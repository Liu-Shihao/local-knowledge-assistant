# æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ

è§£æç”¨æˆ·è¾“å…¥ï¼š
Web URLï¼šhtmlã€gitã€jiraã€
æ–‡ä»¶ï¼šPDFã€TXTæ–‡ä»¶
Imageå›¾åƒ

- é—®ç­”æœºå™¨äººğŸ¤–
- ä»£ç ç†è§£
- å›¾ç‰‡è¯†åˆ«
- å›¾ç‰‡æœç´¢ğŸ”

# Setup

## Ollama
å®˜ç½‘ï¼šhttps://ollama.com/

Windowsç³»ç»ŸæŸ¥çœ‹ollama å¸®åŠ©å‘½ä»¤
```shell
ollama.exe serve --help
```

æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°é»˜è®¤ç›®å½•ï¼š
- On Mac, the models will be download to `~/.ollama/models`
- On Linux (or WSL), the models will be stored at /usr/share/ollama/.ollama/models



### å¸¸ç”¨å‘½ä»¤
```shell
#Pull a model
ollama pull llama2
#Remove a model
ollama rm llama2
#Copy a model
ollama cp llama2 my-llama2
#List models on your computer
ollama list
#æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama show --modelfile mistral
```

### å¦‚ä½•å¯¼å…¥Model
Ollama supports importing GGUF models in the Modelfile:
```shell
#1.Create a file named Modelfile, with a FROM instruction with the local filepath to the model you want to import.
FROM ./vicuna-33b.Q4_0.gguf
#2.Create the model in Ollama
ollama create example -f Modelfile
#3.Run the model
ollama run example
```

###  REST API
Ollama has a REST API for running and managing models.

Generate a response

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt":"Why is the sky blue?"
}'
```
Chat with a model
```shell
curl http://localhost:11434/api/chat -d '{
  "model": "mistral",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'
```